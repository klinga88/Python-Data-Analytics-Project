{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import tweepy\n",
    "import json\n",
    "import numpy as np\n",
    "from config import consumer_key, consumer_secret, access_token, access_token_secret\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import and Initialize Sentiment Analyzer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "analyzer = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Twitter API Keys\n",
    "consumer_key = consumer_key\n",
    "consumer_secret = consumer_secret\n",
    "access_token = access_token\n",
    "access_token_secret = access_token_secret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Tweepy API Authentication\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth, parser=tweepy.parsers.JSONParser())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists to hold sentiments\n",
    "compound_list = []\n",
    "positive_list = []\n",
    "negative_list = []\n",
    "neutral_list = []\n",
    "tweet_text = []\n",
    "tweet_times = []\n",
    "tweet_id = []\n",
    "time_diffs = []\n",
    "tweet_user = []\n",
    "tweet_handle = []\n",
    "tweet_followers = []\n",
    "tweet_language = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_stocks = pd.read_csv(\"NYSE_20180622_top50.csv\")\n",
    "target_terms = top_stocks[\"Description\"].tolist()\n",
    "#target_terms = [\"#Cloudera\",\"Arena Pharmaceuticals\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_between_tweets = []\n",
    "tweet_searchterm = []\n",
    "\n",
    "for term in target_terms:\n",
    "    \n",
    "    public_tweets = api.search(term,count = 100, lang=\"en\")\n",
    "    \n",
    "    # Loop through all tweets\n",
    "    for tweet in public_tweets[\"statuses\"]:\n",
    "        tweet_searchterm.append(term)\n",
    "        print(term)\n",
    "        tweet_text.append(tweet[\"text\"])\n",
    "        tweet_id.append(tweet[\"id\"])\n",
    "        print(tweet[\"id\"])\n",
    "        tweet_user.append(tweet[\"user\"][\"name\"])\n",
    "        tweet_handle.append(tweet[\"user\"][\"screen_name\"])\n",
    "        tweet_followers.append(tweet[\"user\"][\"followers_count\"])\n",
    "        tweet_language.append(tweet[\"user\"][\"lang\"])\n",
    "        tweet_times.append(tweet[\"created_at\"])\n",
    "        \n",
    "        \n",
    "\n",
    "        # Run Vader Analysis on each tweet\n",
    "        compound = analyzer.polarity_scores(tweet[\"text\"])[\"compound\"]\n",
    "        compound_list.append(compound)\n",
    "        \n",
    "        pos = analyzer.polarity_scores(tweet[\"text\"])[\"pos\"]\n",
    "        positive_list.append(pos)\n",
    "        \n",
    "        neu = analyzer.polarity_scores(tweet[\"text\"])[\"neu\"]\n",
    "        neutral_list.append(neu)\n",
    "        \n",
    "        neg = analyzer.polarity_scores(tweet[\"text\"])[\"neg\"]\n",
    "        negative_list.append(neg)\n",
    " \n",
    "\n",
    "    # Add each datetime object into the array\n",
    "    tweet_time_objects = []\n",
    "    for x in range(len(tweet_times)):\n",
    "        tweet_datetime = datetime.strptime(tweet_times[x], \"%a %b %d %H:%M:%S %z %Y\")\n",
    "        tweet_time_objects.append(tweet_datetime)\n",
    "\n",
    "    # Calculate the time between tweets\n",
    "    time_in_between = []\n",
    "    for x in range(len(tweet_time_objects)-1):\n",
    "        secs_apart = ((tweet_time_objects[x] - tweet_time_objects[x+1]).seconds) \n",
    "        time_in_between.append(secs_apart)    \n",
    "         \n",
    "    #add one more row to allow into dataframe(lengths must match)\n",
    "    time_in_between.append(0)    \n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len( tweet_searchterm))\n",
    "print(len( tweet_user))\n",
    "print(len( tweet_handle))\n",
    "print(len( tweet_followers))\n",
    "print(len(  tweet_language))\n",
    "print(len( tweet_id))\n",
    "print(len(  tweet_text))\n",
    "print(len( tweet_time_objects))\n",
    "print(len( time_in_between))\n",
    "print(len(  compound_list))\n",
    "print(len(  positive_list))\n",
    "print(len(negative_list))\n",
    "print(len(  neutral_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\"Search Term\": term_list,\n",
    "\n",
    "tweet_data = pd.DataFrame({\n",
    "                            'Name':tweet_user,\n",
    "                           'Handle':tweet_handle,\n",
    "                           'Followers':tweet_followers,\n",
    "                           'Language' : tweet_language,\n",
    "                            'Id':tweet_id,\n",
    "                            'Text': tweet_text,\n",
    "                           'Time Stamp':tweet_time_objects,\n",
    "                           'Time Delta (seconds)':time_in_between,\n",
    "                           'Compound' : compound_list,\n",
    "                           'positive': positive_list,\n",
    "                           'negative': negative_list,\n",
    "                           'neutral': neutral_list})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in tweet_data.iterrows():\n",
    "    \n",
    "    tweet_data[\"Date\"] = datetime.date(row[\"Time Stamp\"])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the Average Sentiments\n",
    "sentiment = {\"Compound\": np.mean(compound_list),\n",
    "             \"Positive\": np.mean(positive_list),\n",
    "             \"Neutral\": np.mean(neutral_list),\n",
    "             \"Negative\": np.mean(negative_list)}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoticons_str = r\"\"\"\n",
    "    (?:\n",
    "        [:=;] # Eyes\n",
    "        [oO\\-]? # Nose (optional)\n",
    "        [D\\)\\]\\(\\]/\\\\OpP] # Mouth\n",
    "    )\"\"\"\n",
    " \n",
    "regex_str = [\n",
    "    emoticons_str,\n",
    "    r'<[^>]+>', # HTML tags\n",
    "    r'(?:@[\\w_]+)', # @-mentions\n",
    "    r\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\", # hash-tags\n",
    "    r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+', # URLs\n",
    " \n",
    "    r'(?:(?:\\d+,?)+(?:\\.?\\d+)?)', # numbers\n",
    "    r\"(?:[a-z][a-z'\\-_]+[a-z])\", # words with - and '\n",
    "    r'(?:[\\w_]+)', # other words\n",
    "    r'(?:\\S)' # anything else\n",
    "]\n",
    "    \n",
    "tokens_re = re.compile(r'('+'|'.join(regex_str)+')', re.VERBOSE | re.IGNORECASE)\n",
    "emoticon_re = re.compile(r'^'+emoticons_str+'$', re.VERBOSE | re.IGNORECASE)\n",
    " \n",
    "def tokenize(s):\n",
    "    return tokens_re.findall(s)\n",
    " \n",
    "def preprocess(s, lowercase=False):\n",
    "    tokens = tokenize(s)\n",
    "    if lowercase:\n",
    "        tokens = [token if emoticon_re.search(token) else token.lower() for token in tokens]\n",
    "    return tokens\n",
    "\n",
    "tweet = 'RT @marcobonzanini: just an example! :D http://example.com #NLP'\n",
    "print(preprocess(tweet))\n",
    "# ['RT', '@marcobonzanini', ':', 'just', 'an', 'example', '!', ':D', 'http://example.com', '#NLP']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_text_breakdown = []\n",
    "tweet_text_id = []\n",
    "\n",
    "for index, row in tweet_data.iterrows():\n",
    "    text_seperate = preprocess(row[\"Text\"])\n",
    "    tweet_text_breakdown.append(text_seperate)\n",
    "    text_seperate_id = row[\"Id\"]\n",
    "    tweet_text_id.append(text_seperate_id)\n",
    "\n",
    "text_breakdown_df = pd.DataFrame({'Id':tweet_text_id,\n",
    "                                   'Text Breakdown':tweet_text_breakdown})\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_data_all = pd.merge(tweet_data,text_breakdown_df[[\"Id\",\"Text Breakdown\"]], on=\"Id\",how=\"left\")\n",
    "\n",
    "tweet_data.to_csv(\"tweet_data_all.csv\")\n",
    "\n",
    "tweet_data_all.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
